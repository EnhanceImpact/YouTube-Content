# -*- coding: utf-8 -*-
"""DecisionTreeClassifier202411.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10GkT1P_vOldHaWt_luvE_VzJ-RQFwDKo

## Import Libraries and Load Data
"""

import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import OneHotEncoder
import pandas as pd

# Load dataset
data = pd.read_csv('/content/UsedCars.csv')

print("----------- View the First Five Rows of the Data --------------")
print(data.head())

print("------------- Data Information Check of Size and Completeness --------------")
print(data.info())

# Specify columns with "Yes" and "No" values to map to binary
binary_cols = ['Purchase', 'Damage']  # Add any other "Yes"/"No" columns here
binary_cols[1]

"""## Transform Data and Divide Data into Training and Testing Sets"""

# Subset or filter to specific columns of interest
data = data[['Year', 'Make', 'Price', 'Number of Owners', 'Use', 'Body', 'Mileage', 'Damage', 'Purchase']]

# Specify columns with "Yes" and "No" values to map to binary
binary_cols = ['Purchase', 'Damage']  # Add any other "Yes"/"No" columns here

# Map binary columns to 0 and 1
for col in binary_cols:
    data[col] = data[col].map({'No': 0, 'Yes': 1})

# Separate features and target
X = data.drop(['Purchase'], axis=1)
y = data['Purchase']

# Define categorical columns to one-hot encode (excluding binary-encoded columns)
categorical_cols = ['Make', 'Use', 'Body']
numeric_cols = ['Year', 'Price', 'Number of Owners', 'Mileage']

# One-hot encode the categorical columns
encoder = OneHotEncoder()
X_encoded = encoder.fit_transform(X[categorical_cols])

# # Convert the encoded data to a DataFrame with feature names
encoded_feature_names = encoder.get_feature_names_out(categorical_cols)
X_encoded_df = pd.DataFrame(X_encoded.toarray(), columns=encoded_feature_names, index=X.index)

# # Concatenate the encoded categorical data with the numeric and binary columns
X_final = pd.concat([X[numeric_cols + [binary_cols[1]]], X_encoded_df], axis=1)

print("-------------Input Data After Transformations ------------")
print(X_final.head())
# # Continue with splitting the data, training, and evaluating the model as before
# X_train, X_test, y_train, y_test = train_test_split(X_final, y, test_size=0.3, random_state=42)

X_encoded_df.head()

"""## Train Decision Trees with Varying Max Depth"""

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_final, y, test_size=0.3, random_state=42)

# Store accuracies
train_accuracies = []
test_accuracies = []

# Convert column names to strings
X_train.columns = X_train.columns.astype(str)
X_test.columns = X_test.columns.astype(str)
# Vary max_depth from 1 to 20
depths = range(1, 21)
for depth in depths:
    # Create a DecisionTreeClassifier with varying max_depth
    clf = DecisionTreeClassifier(max_depth=depth, random_state=42)

    # Fit the model
    clf.fit(X_train, y_train)

    # Predict on both train and test sets
    y_train_pred = clf.predict(X_train)
    y_test_pred = clf.predict(X_test)

    # Compute accuracies
    train_accuracies.append(accuracy_score(y_train, y_train_pred))
    test_accuracies.append(accuracy_score(y_test, y_test_pred))

# Plotting accuracies
plt.figure(figsize=(8, 6))
plt.plot(depths, train_accuracies, label='Training Accuracy', marker='o')
plt.plot(depths, test_accuracies, label='Testing Accuracy', marker='o')

# Adding labels and title
plt.xlabel('Max Depth')
plt.ylabel('Accuracy')
plt.title('Accuracy vs Max Depth for Decision Tree')
plt.legend()
plt.grid(True)
plt.show()

from sklearn.tree import export_graphviz
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import graphviz
import matplotlib.pyplot as plt

# Initialize and fit the model with the best max_depth
clf_best = DecisionTreeClassifier(max_depth=5,min_samples_split=6,
        min_samples_leaf=10,random_state=42)
clf_best.fit(X_train, y_train)

# Export as DOT file for visualization
dot_data = export_graphviz(
    clf_best, out_file=None,
    feature_names=X_train.columns,
    class_names=[str(class_) for class_ in clf_best.classes_],
    filled=True, rounded=True,
    special_characters=True
)

# Use Graphviz to display the tree
graph = graphviz.Source(dot_data)
graph.render("decision_tree")  # Save the plot as a file if needed
graph  # Display the plot

from sklearn.metrics import ConfusionMatrixDisplay

# Predict on the test set
y_test_pred = clf_best.predict(X_test)

# Generate the confusion matrix
cm = confusion_matrix(y_test, y_test_pred)

# Display the confusion matrix
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=clf_best.classes_)
disp.plot(cmap=plt.cm.Blues)
plt.title("Confusion Matrix for Decision Tree (max_depth=5)")
plt.show()

from sklearn.metrics import accuracy_score

# Calculate training and test accuracy
y_train_pred = clf_best.predict(X_train)
y_test_pred = clf_best.predict(X_test)

train_accuracy = accuracy_score(y_train, y_train_pred)
test_accuracy = accuracy_score(y_test, y_test_pred)

# Print the accuracies
print(f"Training Accuracy: {train_accuracy:.2f}")
print(f"Test Accuracy: {test_accuracy:.2f}")

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import OneHotEncoder
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score

# Load dataset
data = pd.read_csv('/content/UsedCars.csv')
X = data.drop(['Purchase', 'Model'], axis=1)
y = data['Purchase']

# Define categorical and numeric columns
categorical_cols = ['Make', 'Use', 'Body', 'Accident', 'Damage']
numeric_cols = ['Year', 'Price', 'Number of Owners', 'Mileage']

# One-hot encode the categorical columns
encoder = OneHotEncoder()
X_encoded = encoder.fit_transform(X[categorical_cols])

# Convert the encoded data to a DataFrame with feature names
encoded_feature_names = encoder.get_feature_names_out(categorical_cols)
X_encoded_df = pd.DataFrame(X_encoded.toarray(), columns=encoded_feature_names, index=X.index)

# Concatenate the encoded categorical data with the numeric data
X_final = pd.concat([X[numeric_cols], X_encoded_df], axis=1)

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_final, y, test_size=0.3, random_state=42)

# Define parameter ranges
depths = range(1, 21)
min_samples_splits = [2, 5, 10]
min_samples_leaves = [1, 5, 10]

# Store results
best_score = 0
best_params = {}

for depth in depths:
    for min_samples_split in min_samples_splits:
        for min_samples_leaf in min_samples_leaves:
            # Create the Decision Tree with current parameters
            clf = DecisionTreeClassifier(
                max_depth=depth,
                min_samples_split=min_samples_split,
                min_samples_leaf=min_samples_leaf,
                random_state=42
            )

            # Perform cross-validation
            cv_scores = cross_val_score(clf, X_train, y_train, cv=5, scoring='accuracy')
            mean_cv_score = np.mean(cv_scores)

            # Update the best score and parameters if current combination is better
            if mean_cv_score > best_score:
                best_score = mean_cv_score
                best_params = {
                    'max_depth': depth,
                    'min_samples_split': min_samples_split,
                    'min_samples_leaf': min_samples_leaf
                }

# Train final model with best parameters on the entire training set
best_clf = DecisionTreeClassifier(**best_params, random_state=42)
best_clf.fit(X_train, y_train)

# Evaluate on training and testing sets
train_accuracy = accuracy_score(y_train, best_clf.predict(X_train))
test_accuracy = accuracy_score(y_test, best_clf.predict(X_test))

# Print the best parameters and accuracies
print("Best Parameters:")
print(f"Max Depth: {best_params['max_depth']}")
print(f"Min Samples Split: {best_params['min_samples_split']}")
print(f"Min Samples Leaf: {best_params['min_samples_leaf']}")
print(f"Cross-Validated Training Accuracy: {train_accuracy}")
print(f"Testing Accuracy: {test_accuracy}")

