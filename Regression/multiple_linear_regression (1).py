# -*- coding: utf-8 -*-
"""Multiple Linear Regression.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11Wtw2sAl1B5OQAH7ul2Pay6Mhx3IGDz6
"""

# Load Libraries
import pandas as pd
import numpy as np
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_squared_error
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer

# Load data
df = pd.read_csv('/content/Medical_Insurance_Cost_Linear_Regression_Data.csv')

# View first five rows of dataset
df.head()

# Look at data completeness and check the schema/data type correctness
df.info()

# View summaries of the numeric features
df.describe()

df.select_dtypes(exclude='object').head()

# Calculate the correlation coefficient for viewing the strength and direction of linear relationship between features
df.select_dtypes(exclude='object').corr()

# Visual inspection of linear relationship between features
sns.pairplot(df.select_dtypes(exclude='object'))

df.select_dtypes(include='object').head()

# List the categorical feature columns
categorical_features = df.select_dtypes(include='object').columns.to_list()
categorical_features

# Print the categorical feature levels. When there are a lot of levels for a categorical feature try regrouping.
for col in categorical_features:
  print(col, df[col].unique())

# Separate the features and the target variable
X = df.drop('charges', axis=1)
y = df['charges']

# Perform one hot encoding for categorical variables
onehot_encoder = OneHotEncoder(drop='first')

# Transform the data by applying the column transformer model
transformer = ColumnTransformer([('onehot', onehot_encoder, categorical_features)], remainder ='passthrough')

# Encode the categorical features and keeping the numeric features the same
X_encoded = transformer.fit_transform(X)

# Get the column names
transformed_feature_names = transformer.get_feature_names_out()

# Convert the transformer to a data frame for viewing
X_encoded_df = pd.DataFrame(X_encoded, columns = transformed_feature_names)

# View the first five rows of the transformed data
X_encoded_df.head()

# Split the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size = 0.30, random_state = 413)

# Instantiate the model
model = LinearRegression()
# Fit the model on the training data
regr = model.fit(X_train, y_train)
# Make prediction of charges for the given the X features for the training and test data sets
y_pred_train = regr.predict(X_train)
y_pred_test = regr.predict(X_test)
# Compare the actual charges with the predicted charges for the both the training and test data
# r-squared
print("Train r-squared score: ", r2_score(y_train,y_pred_train))
print("Test r-squared score: ", r2_score(y_test,y_pred_test))
# mean_squared_error
print("Train MSE: ", mean_squared_error(y_train,y_pred_train))
print("Test MSE: ", mean_squared_error(y_test,y_pred_test))

for feature, coef in zip(X_encoded_df.columns, regr.coef_):
    print(f"{feature}: {round(coef,1)}")

